# This is the global configuration file for Workflow

global:
  # Admin email, used for each component to send email to administrator
  email: "drycc@drycc.cc"
  # Whether to use the template defined by common
  common: true
  # Set the storage backend
  #
  # Valid values are:
  # - s3: Store persistent data in AWS S3 (configure in S3 section)
  # - azure: Store persistent data in Azure's object storage
  # - gcs: Store persistent data in Google Cloud Storage
  # - oss: Store persistent data in Aliyun OSS
  # - minio: Store persistent data on in-cluster Minio server
  storage: minio

  # Set the location of Workflow's PostgreSQL database
  #
  # Valid values are:
  # - on-cluster: Run PostgreSQL within the Kubernetes cluster (credentials are generated
  #   automatically; backups are sent to object storage
  #   configured above)
  # - off-cluster: Run PostgreSQL outside the Kubernetes cluster (configure in database section)
  databaseLocation: "on-cluster"

  # Set the location of Workflow's Redis instance
  #
  # Valid values are:
  # - on-cluster: Run Redis within the Kubernetes cluster
  # - off-cluster: Run Redis outside the Kubernetes cluster (configure in redis section)
  redisLocation: "on-cluster"
  # Set the location of Workflow's rabbitmq instance
  # Valid values are:
  # - on-cluster: Run Rabbitmq within the Kubernetes cluster
  # - off-cluster: Run Rabbitmq outside the Kubernetes cluster (configure in controller section)
  rabbitmqLocation: "on-cluster"
  # Set the location of Workflow's influxdb cluster
  #
  # Valid values are:
  # - on-cluster: Run Influxdb within the Kubernetes cluster
  # - off-cluster: Influxdb is running outside of the cluster and credentials and connection information will be provided.
  influxdbLocation: "on-cluster"
  # Set the location of Workflow's grafana instance
  #
  # Valid values are:
  # - on-cluster: Run Grafana within the Kubernetes cluster
  # - off-cluster: Grafana is running outside of the cluster
  grafanaLocation: "on-cluster"

  # Set the location of Workflow's Registry
  #
  # Valid values are:
  # - on-cluster: Run registry within the Kubernetes cluster
  # - off-cluster: Use registry outside the Kubernetes cluster (example: dockerhub,quay.io,self-hosted)
  registryLocation: "on-cluster"
  # Prefix for the imagepull secret created when using private registry
  registrySecretPrefix: "private-registry"
  # The host port to which registry proxy binds to
  registryProxyPort: 5555

  # Enable usage of RBAC authorization mode
  #
  # Valid values are:
  # - true: all RBAC-related manifests will be installed (in case your cluster supports RBAC)
  # - false: no RBAC-related manifests will be installed
  rbac: true
  # Please check `kubernetes.io/ingress.class`
  # The cert-manager component must be installed
  # If you want to use HTTPSEnforced or allowlist functions, you must specify:
  # - nginx
  # - traefik
  # Only the above options have been supported so far
  ingressClass: ""
  # A domain name consists of one or more parts.
  # Periods (.) are used to separate these parts.
  # Each part must be 1 to 63 characters in length and can contain lowercase letters, digits, and hyphens (-).
  # It must start and end with a lowercase letter or digit.
  clusterDomain: "cluster.local"
  # The publicly resolvable hostname to build your cluster with.
  #
  # This will be the hostname that is used to build endpoints such as "drycc.$HOSTNAME"
  platformDomain: ""
  # Whether cert_manager is enabled to automatically generate drycc certificates
  certManagerEnabled: true
  # Set the location of Workflow's passport
  #
  # Valid values are:
  # - on-cluster: Run passport within the Kubernetes cluster
  # - off-cluster: Use passport outside the Kubernetes cluster
  passportLocation: "on-cluster"

s3:
  # Your AWS access key. Leave it empty if you want to use IAM credentials.
  accesskey: ""
  # Your AWS secret key. Leave it empty if you want to use IAM credentials.
  secretkey: ""
  # Any S3 region
  endpoint: "your-s3-endpoint"
  # Your buckets.
  builderBucket: "your-builder-bucket-name"
  registryBucket: "your-registry-bucket-name"
  databaseBucket: "your-database-bucket-name"

oss:
  accesskey: ""
  secretkey: ""
  endpoint: "your-oos-endpoint"
  builderBucket: "your-builder-bucket-name"
  registryBucket: "your-registry-bucket-name"
  databaseBucket: "your-database-bucket-name"

azure:
  accesskey: "YOUR ACCOUNT NAME"
  secretkey: "YOUR ACCOUNT KEY"
  endpoint: "your-azure-endpoint"
  builderBucket: "your-builder-container-name"
  registryBucket: "your-registry-container-name"
  databaseBucket: "your-database-container-name"

gcs:
  # keyJson is expanded into a JSON file on the remote server. It must be
  # well-formatted JSON data.
  accesskey: "your accessKey for minio gateway"
  secretkey: "your secretkey for minio gateway"
  keyJson: <base64-encoded JSON data>
  projectid: "your-gcs-projectid"
  builderBucket: "your-builder-bucket-name"
  registryBucket: "your-registry-bucket-name"
  databaseBucket: "your-database-bucket-name"

minio:
  accesskey: "8TZRY2JRWMPT6UMXR6I5"
  secretkey: "gbstrOvotMMcg2sMfGUhA5a6Et/EI5ALtIHsobYk"
  builderBucket: "0fad60517a9e4225a07d77d7b9b58629"
  registryBucket: "c2da53f11c7a455db35797c3ba625d80"
  databaseBucket: "e95eb5c54c3a4047abe5269a68fa9a67"
  # Configure the following ONLY if you want persistence for on-cluster minio
  persistence:
    enabled: false # Set to true to enable persistence
    accessMode: ReadWriteOnce
    size: 5Gi # PVC size
    ## minio data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    storageClass: ""
    volumeName: ""

# Set the default (global) way of how Application (your own) images are
# pulled from within the Controller.
# This can be configured per Application as well in the Controller.
#
# This affects pull apps and git push apps
#
# Values values are:
# - Always
# - IfNotPresent
builder:
  service:
    # Service type default to ClusterIP, the entrance will be taken over by router.
    type: ClusterIP
    # If service.type is not set to NodePort, the following statement will be ignored.
    nodePort: ""

#
passport:
  # Set passport deployment replicas
  replicas: 1
  # Configuring this will no longer use the built-in database component
  databaseUrl: ""

controller:
  appImagePullPolicy: "IfNotPresent"
  # Possible values are:
  # enabled - allows for open registration
  # disabled - turns off open registration
  # admin_only - allows for registration by an admin only.
  registrationMode: "admin_only"
  # Set storageClassName, It is used for application mount.
  appStorageClass: ""
  # Set controller deployment  replicas
  replicas: 1
  # Set celery replicas
  celeryReplicas: 1
  # Configuring this will no longer use the built-in database component
  databaseUrl: ""

database:
  # The username and password to be used by the on-cluster database.
  # If left empty they will be generated using randAlphaNum
  username: ""
  password: ""
  persistence:
    enabled: false # Set to true to enable persistence
    accessMode: ReadWriteOnce
    size: 5Gi # PVC size
    ## database data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    storageClass: ""
    volumeName: ""

redis:
  # The following parameters are configured only when using an on-cluster Redis instance
  replicas: 1
  # The following parameters are configured only when using an off-cluster Redis instance
  addrs: "" # A list of clusters: "127.0.0.1:7001/1,127.0.0.2:7002/1"
  password: "redis password" # "" == no password
  persistence:
    enabled: false # Set to true to enable persistence
    size: 5Gi
    storageClass: ""

rabbitmq:
  # Configure the following ONLY if using an off-cluster rabbitmq
  url: "amqp://myuser:mypassword@localhost:5672/myvhost"
  # The username and password to be used by the on-cluster database.
  # If left empty they will be generated using randAlphaNum
  username: ""
  password: ""
  # GCP PDs and EBS volumes are supported only
  persistence:
    enabled: false # Set to true to enable persistence
    size: 5Gi # PVC size
    ## rabbitmq data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    storageClass: ""

influxdb:
  # URL configuration is only available in off-cluster Influx database
  url: "http://my.influx.url:8086"
  bucket: "kubernetes"
  org: "org"
  # If it is off-cluster, it must be specified
  # Otherwise, it is generated randomly, if it is empty
  token: ""
  # Configure the following is only available in on-cluster Influx database
  user: "admin"
  password: "admin123"
  retention: "30d"
  # GCP PDs and EBS volumes are supported only
  persistence:
    enabled: false # Set to true to enable persistence
    accessMode: ReadWriteOnce
    size: 5Gi # PVC size
    ## influxdb data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    storageClass: ""
    volumeName: ""

fluentd:
  syslog:
    # Configure the following ONLY if using Fluentd to send log messages to both
    # the Logger component and external syslog endpoint
    # external syslog endpoint url
    host: ""
    # external syslog endpoint port
    port: ""

monitor:
  grafana:
    # Configure the following ONLY if you want persistence for on-cluster grafana
    # GCP PDs and EBS volumes are supported only
    persistence:
      enabled: false # Set to true to enable persistence
      accessMode: ReadWriteOnce
      size: 5Gi # PVC size
      ## influxdb data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      ## Storage class of PV to bind. By default it looks for standard storage class.
      ## If the PV uses a different storage class, specify that here.
      storageClass: ""
      volumeName: ""

registry:
  hostname: ""
  organization: ""
  username: ""
  password: ""
